{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3923a5a1-f635-45cf-95b4-6ff72fcda388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le répertoire actuel est : /Users/pouplesse/Documents/Projet - RAG\n",
      "Le répertoire actuel est maintenant : /Users/pouplesse/Documents/Projet - RAG\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "print(\"Le répertoire actuel est :\", current_directory)\n",
    "\n",
    "new_directory = '/Users/pouplesse/Documents/Projet - RAG/'\n",
    "os.chdir(new_directory)\n",
    "print(\"Le répertoire actuel est maintenant :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd11f47a-ed55-4a66-9801-80f382731f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content saved in progrès.txt\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"fr\")\n",
    "\n",
    "page_title = \"Progrès\"\n",
    "\n",
    "pg = wikipedia.page(page_title)\n",
    "\n",
    "# save the content\n",
    "\n",
    "output_file = page_title.replace(\" \", \"_\").replace(\"'\", \"\").lower() + \".txt\"\n",
    "\n",
    "with open(f\"{output_file}\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(pg.content)\n",
    "\n",
    "print(f\"content saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0231966c-6e7d-496d-85db-09b225e1066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loading sources/progrès_accéléré.txt\n",
      "extracted 20 allchunks\n",
      "-- loading sources/hartmut_rosa.txt\n",
      "extracted 27 allchunks\n",
      "-- loading sources/progrès.txt\n",
      "extracted 267 allchunks\n",
      "-- loading sources/progrès_technique.txt\n",
      "extracted 66 allchunks\n",
      "-- count tokens\n",
      "max number of tokens: 838\n",
      "distribution of number of tokens: count    380.000000\n",
      "mean     224.539474\n",
      "std      151.418956\n",
      "min       14.000000\n",
      "25%      106.000000\n",
      "50%      191.000000\n",
      "75%      313.250000\n",
      "max      838.000000\n",
      "Name: token_count, dtype: float64\n",
      "-- saved to sources/extracted_wikipedia_chunks.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chunk the text over line returns with overlap and window_size parameters\n",
    "saves content to a json format\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import typing as t\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def chunkit(input_: t.List[str], window_size: int = 3, overlap: int = 1) -> t.List[str]:\n",
    "    assert (\n",
    "        overlap < window_size\n",
    "    ), f\"overlap {overlap} needs to be smaller than window size {window_size}\"\n",
    "    start_ = 0\n",
    "    chunks = []\n",
    "    while start_ + window_size < len(input_):\n",
    "        chunks.append(input_[start_ : start_ + window_size])\n",
    "        start_ = start_ + window_size - overlap\n",
    "    # add the remaining paragraphs\n",
    "    if start_ < len(input_):\n",
    "        chunks.append(input_[start_ - len(input_) :])\n",
    "\n",
    "    extracts = [\"\\n\".join(chk) for chk in chunks]\n",
    "\n",
    "    return extracts\n",
    "\n",
    "\n",
    "def test_chunkit() -> None:\n",
    "    # array of the alphabet\n",
    "    input_ = [l for l in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "    output_ = chunkit(input_, window_size=5, overlap=2)\n",
    "    assert len(output_) == 8\n",
    "    output_ = chunkit(input_, window_size=2, overlap=1)\n",
    "    assert len(output_) == 25\n",
    "\n",
    "\n",
    "test_chunkit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_path = \"sources/*.txt\"\n",
    "    source_files = glob.glob(source_path)\n",
    "\n",
    "    allchunks = []\n",
    "    for filename in source_files:\n",
    "        print(f\"-- loading {filename}\")\n",
    "        with open(filename, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        # split the text over line returns\n",
    "        lines = txt.split(\"\\n\")\n",
    "        # remove empty lines\n",
    "        lines = [par.strip() for par in lines if len(par.strip()) > 1]\n",
    "        chunked_version = chunkit(lines)\n",
    "\n",
    "        allchunks += chunked_version\n",
    "\n",
    "        print(f\"extracted {len(chunked_version)} allchunks\")\n",
    "\n",
    "    # set as dataframe, to make it easier to add other info for each chunk and save it to json later on\n",
    "    data = pd.DataFrame(data=allchunks, columns=[\"text\"])\n",
    "\n",
    "    # create unique id for each chunk\n",
    "    data[\"uuid\"] = [str(uuid.uuid4()) for i in range(len(data))]\n",
    "\n",
    "    # count the number of tokens\n",
    "    print(\"-- count tokens\")\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    data[\"token_count\"] = data.text.apply(lambda txt: len(encoding.encode(txt)))\n",
    "\n",
    "    # check the max number of tokens in the dataset\n",
    "    print(f\"max number of tokens: {max(data.token_count)}\")\n",
    "    print(f\"distribution of number of tokens: {data.token_count.describe()}\")\n",
    "\n",
    "    # save to json\n",
    "    output_file_json = \"sources/extracted_wikipedia_chunks.json\"\n",
    "    with open(output_file_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        data.to_json(f, force_ascii=False, orient=\"records\", indent=4)\n",
    "\n",
    "    print(f\"-- saved to {output_file_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c91e68b7-06ae-46fb-945f-baafddae993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde clef d'acces Weaviate\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "os.environ[\"WEAVIATE_KEY\"] = \"TRroU187hKQOK5kQKdoNIBeolVyMQd8Mxjxg\"\n",
    "os.environ[\"WEAVIATE_CLUSTER_URL\"] = \"https://6q45jbvsfo4cmhzrpjwoq.c1.europe-west3.gcp.weaviate.cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff4af4f-4d22-45c8-8a15-9ea5871f7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifier que la clé a bien été enregistrée\n",
    "assert os.environ.get('OPENAI_API_KEY')  is not None\n",
    "assert os.environ.get('WEAVIATE_KEY')  is not None\n",
    "assert os.environ.get('WEAVIATE_CLUSTER_URL') is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f2df4d-ded1-4f50-aafa-c000c0b16f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "def connect_to_weaviate() -> weaviate.client.WeaviateClient:\n",
    "    client = weaviate.connect_to_wcs(\n",
    "        cluster_url=os.environ[\"WEAVIATE_CLUSTER_URL\"],\n",
    "        auth_credentials=weaviate.AuthApiKey(os.environ[\"WEAVIATE_KEY\"]),\n",
    "        headers={\n",
    "            \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        },\n",
    "    )\n",
    "    # check that the vector store is up and running\n",
    "    if client.is_live() & client.is_ready() & client.is_connected():\n",
    "        print(f\"client is live, ready and connected \")\n",
    "\n",
    "    assert (\n",
    "        client.is_live() & client.is_ready()\n",
    "    ), \"Weaviate client is not live or not ready or not connected\"\n",
    "    return client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7736c4-388b-41e5-9ddd-eec4d8b058b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pouplesse/anaconda3/lib/python3.10/site-packages/weaviate/__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client is live, ready and connected \n"
     ]
    }
   ],
   "source": [
    "client = connect_to_weaviate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6eff9d3-a36b-4d10-bba7-89abbe51a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Reconfigure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aedb99bc-4a15-4cd7-9ed2-14a4d391c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- loaded  380 items\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_file = \"sources/extracted_wikipedia_chunks.json\"\n",
    "data = pd.read_json(input_file)\n",
    "data = data[[\"uuid\", \"text\"]]\n",
    "print(\"-- loaded \", data.shape[0], \"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30d232ad-59d6-45ce-8624-56ab8144d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"Ludivine_progres_mars_2024\"\n",
    "collection = client.collections.get(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9af200a-7eb1-4664-a250-aad40ce8a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the data\n",
    "batch_result = collection.data.insert_many(data.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b174d26-4f67-472b-8f40-647bdddf7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch_result.has_errors:\n",
    "    print(batch_result.errors)\n",
    "    raise RuntimeError(\"stopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88316a62-f16a-4a92-8461-ea09a67a70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection Ludivine_progres_mars_2024 now has 1140 records\n"
     ]
    }
   ],
   "source": [
    "collection = client.collections.get(collection_name)\n",
    "\n",
    "records_num = collection.aggregate.over_all(total_count=True).total_count\n",
    "print(f\"collection {collection_name} now has {records_num} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e78ab0b5-e5cd-46e7-a776-2047cc95a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = [\n",
    "    Property(\n",
    "        name=\"uuid\",\n",
    "        data_type=DataType.UUID,\n",
    "        skip_vectorization=True,\n",
    "        vectorize_property_name=False,\n",
    "    ),\n",
    "    Property(\n",
    "        name=\"text\",\n",
    "        data_type=DataType.TEXT,\n",
    "        skip_vectorization=False,\n",
    "        vectorize_property_name=False,\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a099da66-a0a8-4fbc-846b-ffb5026915e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Configure.Vectorizer.text2vec_openai(\n",
    "    vectorize_collection_name=False, model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1907f205-450e-431f-aa6d-b6dea04f1c55",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Ludivine_progres_mars_2024 (exists True)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m all_existing_collections \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcollections\u001b[38;5;241m.\u001b[39mlist_all()\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m      4\u001b[0m collection_exists \u001b[38;5;241m=\u001b[39m collection_name \u001b[38;5;129;01min\u001b[39;00m all_existing_collections\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m collection_exists, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (exists \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_exists\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Ludivine_progres_mars_2024 (exists True)"
     ]
    }
   ],
   "source": [
    "# create collection\n",
    "# 1st check if collection does not exist\n",
    "all_existing_collections = client.collections.list_all().keys()\n",
    "collection_exists = collection_name in all_existing_collections\n",
    "assert not collection_exists, f\"{collection_name} (exists {collection_exists})\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e69ed6da-789d-44c4-819f-fbedaae089a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedStatusCodeError",
     "evalue": "Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"Ludivine_progres_mars_2024\" already exists'}]}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now create the collection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/weaviate/collections/collections.py:130\u001b[0m, in \u001b[0;36m_Collections.create\u001b[0;34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateInvalidInputError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid collection config create parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     config\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name\n\u001b[1;32m    133\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of created collection (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match given name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    135\u001b[0m     name,\n\u001b[1;32m    136\u001b[0m     data_model_properties,\n\u001b[1;32m    137\u001b[0m     data_model_references,\n\u001b[1;32m    138\u001b[0m     skip_argument_validation\u001b[38;5;241m=\u001b[39mskip_argument_validation,\n\u001b[1;32m    139\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/weaviate/collections/base.py:59\u001b[0m, in \u001b[0;36m_CollectionsBase._create\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     57\u001b[0m     config: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/schema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweaviate_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCollection may not have been created properly.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ExpectedStatusCodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mok_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCreate collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     collection_name \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(collection_name, \u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/weaviate/connect/v4.py:477\u001b[0m, in \u001b[0;36m_Connection.post\u001b[0;34m(self, path, weaviate_object, params, error_msg, status_codes)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    471\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m     status_codes: Optional[_ExpectedStatusCodes] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    476\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__send\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_version_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweaviate_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweaviate_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_msg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatus_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/weaviate/connect/v4.py:428\u001b[0m, in \u001b[0;36m_Connection.__send\u001b[0;34m(self, method, url, error_msg, status_codes, weaviate_object, params)\u001b[0m\n\u001b[1;32m    426\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(req)\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status_codes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m status_codes\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedStatusCodeError(error_msg, response\u001b[38;5;241m=\u001b[39mres)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Response, res)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mUnexpectedStatusCodeError\u001b[0m: Collection may not have been created properly.! Unexpected status code: 422, with response body: {'error': [{'message': 'class name \"Ludivine_progres_mars_2024\" already exists'}]}."
     ]
    }
   ],
   "source": [
    "# now create the collection\n",
    "collection = client.collections.create(\n",
    "    name=collection_name,\n",
    "    vectorizer_config=vectorizer,\n",
    "    properties=properties,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689d485a-ba0d-42d8-bd48-9e318beee96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add French stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pouplesse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "print(\"add French stopwords\")\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "collection.config.update(\n",
    "    # Note, use Reconfigure here (not Configure)\n",
    "    inverted_index_config=Reconfigure.inverted_index(\n",
    "        stopwords_additions=list(stopwords.words(\"french\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "879d5a1c-b75b-4fb8-9b0e-9cebde1061b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ludivine_progres_mars_2024 has been created\n"
     ]
    }
   ],
   "source": [
    "all_existing_collections = client.collections.list_all().keys()\n",
    "\n",
    "if collection_name in all_existing_collections:\n",
    "    print(f\"{collection_name} has been created\")\n",
    "else:\n",
    "    print(f\"{collection_name} has NOT been created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37cad385-fff9-47dc-a4ba-89cb3583d6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, search_mode = \"hybrid\", response_count = 1):\n",
    "    metadata = [\"distance\", \"certainty\", \"score\", \"explain_score\"]\n",
    "    if search_mode == \"hybrid\":\n",
    "        response = collection.query.hybrid(\n",
    "            query=query,\n",
    "            query_properties=[\"text\"],\n",
    "            limit=response_count,\n",
    "            return_metadata=metadata,\n",
    "        )\n",
    "    elif search_mode == \"near_text\":\n",
    "        response = collection.query.near_text(\n",
    "            query=query,\n",
    "            limit=response_count,\n",
    "            return_metadata=metadata,\n",
    "        )\n",
    "    elif search_mode == \"bm25\":\n",
    "        response = collection.query.bm25(\n",
    "            query=query,\n",
    "            limit=response_count,\n",
    "            return_metadata=metadata,\n",
    "        )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd3b8475-53f8-4deb-b102-8da4024d96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Qui est Hartmut Rosa\"\n",
    "response = search(query, response_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cc61de8-2c9b-4e63-a5cb-731a900491cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "eb411ad7-8729-46f8-882d-49c67afb7cd2\n",
      "\n",
      "Hartmut Rosa, né le 15 août 1965 à Lörrach, est un sociologue, philosophe et universitaire allemand, professeur à l'université Friedrich-Schiller d'Iéna et directeur du Max-Weber-Kolleg (de) à Erfurt. Il fait partie d'une nouvelle génération de penseurs travaillant dans le sillage de la théorie critique (École de Francfort).\n",
      "== Biographie ==\n",
      "Hartmut Rosa fait des études de science politique et philosophie à l'université de Fribourg-en-Brisgau, puis il obtient en 1997 un doctorat en sciences sociales de l'université Humboldt de Berlin. Sa thèse, publiée en 1998, porte sur la philosophie sociale et politique de Charles Taylor. Il réalise un postdoctorat à la London School of Economics.\n",
      "\n",
      "metadata: {'creation_time': None, 'last_update_time': None, 'distance': None, 'certainty': None, 'score': 0.8723698854446411, 'explain_score': '\\nHybrid (Result Set vector) Document 7c39281b-e2d5-4456-aebd-518449f0cee0: original score 0.7369852, normalized score: 0.5 - \\nHybrid (Result Set keyword) Document 7c39281b-e2d5-4456-aebd-518449f0cee0: original score 3.8962336, normalized score: 0.37236986', 'is_consistent': None, 'rerank_score': None}\n",
      "\n",
      "----------------------------------------\n",
      "eb411ad7-8729-46f8-882d-49c67afb7cd2\n",
      "\n",
      "Hartmut Rosa, né le 15 août 1965 à Lörrach, est un sociologue, philosophe et universitaire allemand, professeur à l'université Friedrich-Schiller d'Iéna et directeur du Max-Weber-Kolleg (de) à Erfurt. Il fait partie d'une nouvelle génération de penseurs travaillant dans le sillage de la théorie critique (École de Francfort).\n",
      "== Biographie ==\n",
      "Hartmut Rosa fait des études de science politique et philosophie à l'université de Fribourg-en-Brisgau, puis il obtient en 1997 un doctorat en sciences sociales de l'université Humboldt de Berlin. Sa thèse, publiée en 1998, porte sur la philosophie sociale et politique de Charles Taylor. Il réalise un postdoctorat à la London School of Economics.\n",
      "\n",
      "metadata: {'creation_time': None, 'last_update_time': None, 'distance': None, 'certainty': None, 'score': 0.8721967935562134, 'explain_score': '\\nHybrid (Result Set vector) Document 8deb2d1e-fd19-4975-b5b8-e3ab6abf8c45: original score 0.73680234, normalized score: 0.49982697 - \\nHybrid (Result Set keyword) Document 8deb2d1e-fd19-4975-b5b8-e3ab6abf8c45: original score 3.8962336, normalized score: 0.37236986', 'is_consistent': None, 'rerank_score': None}\n",
      "\n",
      "----------------------------------------\n",
      "dd7bb063-3006-47fd-959a-482f20860988\n",
      "\n",
      "Hartmut Rosa, né le 15 août 1965 à Lörrach, est un sociologue, philosophe et universitaire allemand, professeur à l'université Friedrich-Schiller d'Iéna et directeur du Max-Weber-Kolleg (de) à Erfurt. Il fait partie d'une nouvelle génération de penseurs travaillant dans le sillage de la théorie critique (École de Francfort).\n",
      "== Biographie ==\n",
      "Hartmut Rosa fait des études de science politique et philosophie à l'université de Fribourg-en-Brisgau, puis il obtient en 1997 un doctorat en sciences sociales de l'université Humboldt de Berlin. Sa thèse, publiée en 1998, porte sur la philosophie sociale et politique de Charles Taylor. Il réalise un postdoctorat à la London School of Economics.\n",
      "\n",
      "metadata: {'creation_time': None, 'last_update_time': None, 'distance': None, 'certainty': None, 'score': 0.8721967935562134, 'explain_score': '\\nHybrid (Result Set vector) Document a4cb35ba-38f7-4678-a239-539641e92d02: original score 0.73680234, normalized score: 0.49982697 - \\nHybrid (Result Set keyword) Document a4cb35ba-38f7-4678-a239-539641e92d02: original score 3.8962336, normalized score: 0.37236986', 'is_consistent': None, 'rerank_score': None}\n",
      "\n",
      "----------------------------------------\n",
      "106e8766-91ca-44d1-a965-c4000008bb08\n",
      "\n",
      "L'invité des matins. Rencontre avec Hartmut Rosa, le philosophe anti-moderne . France Culture, 24 janvier 2020\n",
      "[entretien] Youness Bousenna, « Hartmut Rosa, penseur de l’accélération : « L’accélération conduit à un état d’agressivité, particulièrement sensible chez les individus des sociétés occidentales » », Le Monde,‎ 10 septembre 2023 (lire en ligne, consulté le 10 septembre 2023).\n",
      "Ressources relatives à la recherche : Dimensions Geförderte Projekte Informationssystem\n",
      "\n",
      "metadata: {'creation_time': None, 'last_update_time': None, 'distance': None, 'certainty': None, 'score': 0.7743517160415649, 'explain_score': '\\nHybrid (Result Set vector) Document d4a58697-3f65-4371-bb17-f0e9b2f5d5b7: original score 0.57373387, normalized score: 0.34552714 - \\nHybrid (Result Set keyword) Document d4a58697-3f65-4371-bb17-f0e9b2f5d5b7: original score 4.2611556, normalized score: 0.4288246', 'is_consistent': None, 'rerank_score': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in response.objects:\n",
    "    print(\"--\" * 20)\n",
    "    print(item.properties.get(\"uuid\"))\n",
    "    print()\n",
    "    print(item.properties.get(\"text\"))\n",
    "    print()\n",
    "    print(f\"metadata: {item.metadata.__dict__}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1ea37240-94c4-4b73-a865-a319f0d3732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Local\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_generate_groundtruth = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "Prends le rôle d'une journaliste, spécialiste de la notion de progrès\n",
    "Tu dois écrire une question et sa réponse en fonction du contexte.\n",
    "\n",
    "Respecte les consignes suivantes:\n",
    "- formule une question simple, de quelques mots\n",
    "- donne la question et sa réponse au format JSON\n",
    "\n",
    "--------\n",
    "Le contexte:\n",
    "{context}\n",
    "--------\n",
    "\n",
    "    \"question\": \"<la question>\"\n",
    "    \"reponse\": \"<la réponse>\"\n",
    "\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2f0cf924-038f-49d5-80b3-a1432be93e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate(object):\n",
    "    def __init__(self,\n",
    "            model: str = \"gpt-3.5-turbo-0125\",\n",
    "            temperature: float = 0.9\n",
    "        ) -> None:\n",
    "\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "        context_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt= prompt_generate_groundtruth,\n",
    "            output_key=\"output\",\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        self.overall_context_chain = SequentialChain(\n",
    "            chains=[context_chain, ],\n",
    "            input_variables=[\"context\"],\n",
    "            output_variables=[\"output\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    def generate_question_answer(self, context):\n",
    "        response = self.overall_context_chain({\"context\": context})\n",
    "        return response[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8ef5b1f6-46bf-4445-bf83-021e5de4ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_json('sources/extracted_wikipedia_chunks.json')\n",
    "data = data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f5fa534f-be65-4bae-bd2e-fe54c4320c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "0 {\n",
      "    \"question\": \"Le progrès technique s'accélère-t-il ?\",\n",
      "    \"reponse\": \"Oui, plusieurs chercheurs croient en l'augmentation du taux d'accélération du progrès technique au cours de l'histoire.\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1 \n",
      "{\n",
      "    \"question\": \"Qu'est-ce que l'éphémérisation ?\",\n",
      "    \"reponse\": \"La tendance à faire plus avec moins dans divers domaines industriels.\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "2 {\n",
      "    \"question\": \"Le progrès technique est-il en accélération constante ?\",\n",
      "    \"reponse\": \"Oui, selon Stanislaw Ulam, le progrès technique est en accélération constante, ce qui peut conduire à une singularité dans l'histoire de l'espèce.\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "3 {\n",
      "    \"question\": \"Quelle théorie a été développée par Hans Moravec ?\",\n",
      "    \"reponse\": \"Hans Moravec a théorisé le progrès accéléré.\"\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "4 {\n",
      "    \"question\": \"Quelles sont les prédictions de Hans Moravec sur la vie artificielle ?\",\n",
      "    \"reponse\": \"Hans Moravec a développé des prédictions sur la vie artificielle à partir de la loi de Moore dans ses articles et livres.\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "qa = []\n",
    "bad_formatted = []\n",
    "for i, chunk in data.iterrows():\n",
    "\n",
    "    gen = Generate()\n",
    "    answer = gen.generate_question_answer(chunk.text)\n",
    "    print(i, answer)\n",
    "    try:\n",
    "        qa.append(json.loads(answer))\n",
    "    except:\n",
    "        bad_formatted.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22ebc0f4-1822-4b8a-848f-3d910004ee3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': \"Le progrès technique s'accélère-t-il ?\",\n",
       "  'reponse': \"Oui, plusieurs chercheurs croient en l'augmentation du taux d'accélération du progrès technique au cours de l'histoire.\"},\n",
       " {'question': \"Qu'est-ce que l'éphémérisation ?\",\n",
       "  'reponse': 'La tendance à faire plus avec moins dans divers domaines industriels.'},\n",
       " {'question': 'Le progrès technique est-il en accélération constante ?',\n",
       "  'reponse': \"Oui, selon Stanislaw Ulam, le progrès technique est en accélération constante, ce qui peut conduire à une singularité dans l'histoire de l'espèce.\"},\n",
       " {'question': 'Quelle théorie a été développée par Hans Moravec ?',\n",
       "  'reponse': 'Hans Moravec a théorisé le progrès accéléré.'},\n",
       " {'question': 'Quelles sont les prédictions de Hans Moravec sur la vie artificielle ?',\n",
       "  'reponse': 'Hans Moravec a développé des prédictions sur la vie artificielle à partir de la loi de Moore dans ses articles et livres.'}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e22649c-7f84-4a25-915c-927150e2502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.DataFrame(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e6fb5ab3-c646-417e-aa91-8c301ea35d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_json = \"qa_20240312.json\"\n",
    "with open(output_file_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    qa_df.to_json(f, force_ascii=False, orient=\"records\", indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ca11846-e912-42ff-b474-c531d974c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "class Retrieve(object):\n",
    "    collection_name = \"Ludivine_progres_mars_2024\"\n",
    "\n",
    "    def __init__(self, query: str, search_params: t.Dict) -> None:\n",
    "        self.client = connect_to_weaviate()\n",
    "        assert self.client is not None\n",
    "        assert self.client.is_live()\n",
    "\n",
    "        # retrieval\n",
    "        self.collection = self.client.collections.get(Retrieve.collection_name)\n",
    "        self.query = query\n",
    "        self.search_mode = search_params.get(\"search_mode\")\n",
    "        self.response_count = search_params.get(\"response_count\")\n",
    "\n",
    "        # output\n",
    "        self.response = \"\"\n",
    "        self.chunk_texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    # retrieve\n",
    "    def search(self):\n",
    "        metadata = [\"distance\", \"certainty\", \"score\", \"explain_score\"]\n",
    "        if self.search_mode == \"hybrid\":\n",
    "            self.response = self.collection.query.hybrid(\n",
    "                query=self.query,\n",
    "                # query_properties=[\"text\"],\n",
    "                limit=self.response_count,\n",
    "                return_metadata=metadata,\n",
    "            )\n",
    "        elif self.search_mode == \"near_text\":\n",
    "            self.response = self.collection.query.near_text(\n",
    "                query=self.query,\n",
    "                limit=self.response_count,\n",
    "                return_metadata=metadata,\n",
    "            )\n",
    "        elif self.search_mode == \"bm25\":\n",
    "            self.response = self.collection.query.bm25(\n",
    "                query=self.query,\n",
    "                limit=self.response_count,\n",
    "                return_metadata=metadata,\n",
    "            )\n",
    "\n",
    "    def get_context(self):\n",
    "        texts = []\n",
    "        metadata = []\n",
    "        if len(self.response.objects) > 0:\n",
    "            for i in range(min([self.response_count, len(self.response.objects)])):\n",
    "                prop = self.response.objects[i].properties\n",
    "                texts.append(f\"--- \\n{prop.get('text')}\")\n",
    "                metadata.append(self.response.objects[i].metadata)\n",
    "            self.chunk_texts = texts\n",
    "            self.metadata = metadata\n",
    "\n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "\n",
    "    def process(self):\n",
    "        self.search()\n",
    "        self.get_context()\n",
    "        self.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f5d4d4ca-c03a-4d19-8449-0152c97390d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = {\n",
    "        \"search_mode\": \"hybrid\",\n",
    "        \"response_count\": 5,\n",
    "        \"model\": \"gpt-3.5-turbo-0125\",\n",
    "        \"temperature\": 0.5,\n",
    "    }\n",
    "\n",
    "    query = \"Quelle est le problème du progrès ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "16d9a5f1-49aa-402a-aa56-17c15e1976db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client is live, ready and connected \n"
     ]
    }
   ],
   "source": [
    "ret = Retrieve(query, params)\n",
    "ret.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2795ed3-d57c-4840-8244-05dd0856290d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"--- \\nEn 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\\nEn 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\\nEn 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\",\n",
       " \"--- \\nEn 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\\nEn 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\\nEn 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\",\n",
       " \"--- \\nEn 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\\nEn 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\\nEn 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\",\n",
       " \"--- \\nConcrètement, « le progrès » est vécu comme la combinaison des innovations techniques et de l'économie, le processus qui crée une synergie entre ces faits que sont la conception, la fabrication, la commercialisation et l'utilisation de ces innovations. En 1888, l'économiste John Kells Ingram invente l'expression « homo œconomicus » pour exprimer le changement radical qui s'opère alors dans la civilisation occidentale : la révolution industrielle.\\nEn 1948, Jacques Ellul affirmera que c'est parce que « le progrès » constitue un enchevêtrement de faits (et non plus une simple conception philosophique) qu'il est extrêmement difficile de formuler sur lui un avis consensuel :\\n« Sans aucun doute, le motif le plus puissant qui pèse sur nous comme un interdit, le motif qui nous empêche de remettre en question les structures de cette civilisation et de nous lancer dans la voie de la révolution nécessaire, c'est le respect du fait. (...) Actuellement, le fait constitue la raison dernière, le critère de vérité. Il n'y a pas de jugement à porter sur lui, estime t-on, il n'y a qu'à s'incliner. Et dès lors que la technique, l'État ou la production sont des faits, il convient de s'en accommoder. Nous avons là le nœud de la véritable religion moderne : la religion du fait acquis. »\",\n",
       " \"--- \\nConcrètement, « le progrès » est vécu comme la combinaison des innovations techniques et de l'économie, le processus qui crée une synergie entre ces faits que sont la conception, la fabrication, la commercialisation et l'utilisation de ces innovations. En 1888, l'économiste John Kells Ingram invente l'expression « homo œconomicus » pour exprimer le changement radical qui s'opère alors dans la civilisation occidentale : la révolution industrielle.\\nEn 1948, Jacques Ellul affirmera que c'est parce que « le progrès » constitue un enchevêtrement de faits (et non plus une simple conception philosophique) qu'il est extrêmement difficile de formuler sur lui un avis consensuel :\\n« Sans aucun doute, le motif le plus puissant qui pèse sur nous comme un interdit, le motif qui nous empêche de remettre en question les structures de cette civilisation et de nous lancer dans la voie de la révolution nécessaire, c'est le respect du fait. (...) Actuellement, le fait constitue la raison dernière, le critère de vérité. Il n'y a pas de jugement à porter sur lui, estime t-on, il n'y a qu'à s'incliner. Et dès lors que la technique, l'État ou la production sont des faits, il convient de s'en accommoder. Nous avons là le nœud de la véritable religion moderne : la religion du fait acquis. »\"]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.chunk_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "29e6c6bf-923b-4e62-aad8-2179f21809e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.5516278743743896, explain_score='\\nHybrid (Result Set vector) Document a1f24388-9feb-4620-ba32-f5c36eb60d10: original score 0.61426604, normalized score: 0.05162785 - \\nHybrid (Result Set keyword) Document a1f24388-9feb-4620-ba32-f5c36eb60d10: original score 2.971829, normalized score: 0.5', is_consistent=None, rerank_score=None),\n",
       " MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.5516278743743896, explain_score='\\nHybrid (Result Set vector) Document 156bc83d-e6cd-4bb5-aa91-ae923593f3c1: original score 0.61426604, normalized score: 0.05162785 - \\nHybrid (Result Set keyword) Document 156bc83d-e6cd-4bb5-aa91-ae923593f3c1: original score 2.971829, normalized score: 0.5', is_consistent=None, rerank_score=None),\n",
       " MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.5516278743743896, explain_score='\\nHybrid (Result Set vector) Document 3682ee98-52e6-4c9e-a54e-514a645fd636: original score 0.61426604, normalized score: 0.05162785 - \\nHybrid (Result Set keyword) Document 3682ee98-52e6-4c9e-a54e-514a645fd636: original score 2.971829, normalized score: 0.5', is_consistent=None, rerank_score=None),\n",
       " MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.5, explain_score='\\nHybrid (Result Set vector) Document 8c079094-1d96-4f5a-8aa1-be9830668ed1: original score 0.7181373, normalized score: 0.5', is_consistent=None, rerank_score=None),\n",
       " MetadataReturn(creation_time=None, last_update_time=None, distance=None, certainty=None, score=0.5, explain_score='\\nHybrid (Result Set vector) Document 17403d23-1ecc-4eb9-bc08-621a67d206a0: original score 0.7181373, normalized score: 0.5', is_consistent=None, rerank_score=None)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a8efc505-c548-4452-b4e6-fc8dc06fb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_generative_context = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Prends le rôle d'une journaliste spécialisée sur le progrès.\n",
    "Tu écris un article sur l'UE pour le grand public.\n",
    "\n",
    "En tant qu'IA, tu peux utiliser tes connaissances générales pour répondre à la question mais surtout n'invente rien.\n",
    "\n",
    "Indique clairement\n",
    "- Si le contexte ne permet pas de répondre à la question\n",
    "- Si tes connaissances générales ne te permettent pas de répondre à la question\n",
    "\n",
    "Voici une question et un contexte.\n",
    "Réponds à la question en prenant compte l'information dans le contexte.\n",
    "Écris une réponse dans un style concis .\n",
    "\n",
    "--- Le contexte:\n",
    "{context}\n",
    "--- La question:\n",
    "{query}\n",
    "Ta réponse:\n",
    "\"\"\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "78b92b99-4672-4209-bc64-26032e357a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "\n",
    "class Generate(object):\n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo-0125\", temperature: float = 0.5) -> None:\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "\n",
    "        llm_chain = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt_generative_context,\n",
    "            output_key=\"answer\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        self.overall_context_chain = SequentialChain(\n",
    "            chains=[llm_chain],\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "            output_variables=[\"answer\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "        # outputs\n",
    "        self.answer = \"\"\n",
    "\n",
    "    def generate_answer(self, chunk_texts: t.List[str], query: str) -> str:\n",
    "        response_context = self.overall_context_chain(\n",
    "            {\"context\": \"\\n\".join(chunk_texts), \"query\": query}\n",
    "        )\n",
    "        self.answer = response_context[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "76aadb23-0c33-4281-b095-02fd1c83f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelle est le problème du progrès ?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Prends le rôle d'une journaliste spécialisée sur le progrès.\n",
      "Tu écris un article sur l'UE pour le grand public.\n",
      "\n",
      "En tant qu'IA, tu peux utiliser tes connaissances générales pour répondre à la question mais surtout n'invente rien.\n",
      "\n",
      "Indique clairement\n",
      "- Si le contexte ne permet pas de répondre à la question\n",
      "- Si tes connaissances générales ne te permettent pas de répondre à la question\n",
      "\n",
      "Voici une question et un contexte.\n",
      "Réponds à la question en prenant compte l'information dans le contexte.\n",
      "Écris une réponse dans un style concis .\n",
      "\n",
      "--- Le contexte:\n",
      "--- \n",
      "En 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\n",
      "En 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\n",
      "En 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\n",
      "--- \n",
      "En 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\n",
      "En 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\n",
      "En 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\n",
      "--- \n",
      "En 1851, dans Parerga et Paralipomena, Arthur Schopenhauer écrit ces mots : « Le progrès c’est là votre chimère. Il est le rêve du XIXe siècle comme la résurrection était celui du Xe siècle ; chaque âge a le sien ».\n",
      "En 1853, Gustave Flaubert est plus sévère encore : « Ô Lumières, Ô Progrès, Ô humanité ! (...) Quelle éternelle horloge de bêtises que le cours des âges ! (...) C'est une chose curieuse comme l'humanité, à mesure qu'elle se fait autolâtre, devient stupide ».\n",
      "En 1855, le philosophe Eugène Huzar formule la première approche catastrophiste du « progrès »,.\n",
      "--- \n",
      "Concrètement, « le progrès » est vécu comme la combinaison des innovations techniques et de l'économie, le processus qui crée une synergie entre ces faits que sont la conception, la fabrication, la commercialisation et l'utilisation de ces innovations. En 1888, l'économiste John Kells Ingram invente l'expression « homo œconomicus » pour exprimer le changement radical qui s'opère alors dans la civilisation occidentale : la révolution industrielle.\n",
      "En 1948, Jacques Ellul affirmera que c'est parce que « le progrès » constitue un enchevêtrement de faits (et non plus une simple conception philosophique) qu'il est extrêmement difficile de formuler sur lui un avis consensuel :\n",
      "« Sans aucun doute, le motif le plus puissant qui pèse sur nous comme un interdit, le motif qui nous empêche de remettre en question les structures de cette civilisation et de nous lancer dans la voie de la révolution nécessaire, c'est le respect du fait. (...) Actuellement, le fait constitue la raison dernière, le critère de vérité. Il n'y a pas de jugement à porter sur lui, estime t-on, il n'y a qu'à s'incliner. Et dès lors que la technique, l'État ou la production sont des faits, il convient de s'en accommoder. Nous avons là le nœud de la véritable religion moderne : la religion du fait acquis. »\n",
      "--- \n",
      "Concrètement, « le progrès » est vécu comme la combinaison des innovations techniques et de l'économie, le processus qui crée une synergie entre ces faits que sont la conception, la fabrication, la commercialisation et l'utilisation de ces innovations. En 1888, l'économiste John Kells Ingram invente l'expression « homo œconomicus » pour exprimer le changement radical qui s'opère alors dans la civilisation occidentale : la révolution industrielle.\n",
      "En 1948, Jacques Ellul affirmera que c'est parce que « le progrès » constitue un enchevêtrement de faits (et non plus une simple conception philosophique) qu'il est extrêmement difficile de formuler sur lui un avis consensuel :\n",
      "« Sans aucun doute, le motif le plus puissant qui pèse sur nous comme un interdit, le motif qui nous empêche de remettre en question les structures de cette civilisation et de nous lancer dans la voie de la révolution nécessaire, c'est le respect du fait. (...) Actuellement, le fait constitue la raison dernière, le critère de vérité. Il n'y a pas de jugement à porter sur lui, estime t-on, il n'y a qu'à s'incliner. Et dès lors que la technique, l'État ou la production sont des faits, il convient de s'en accommoder. Nous avons là le nœud de la véritable religion moderne : la religion du fait acquis. »\n",
      "--- La question:\n",
      "Quelle est le problème du progrès ?\n",
      "Ta réponse:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "gen = Generate()\n",
    "gen.generate_answer(ret.chunk_texts, query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bf1af987-78f5-41f0-bba3-a054acca272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le problème du progrès réside dans le fait qu'il est devenu un enchevêtrement de faits concrets plutôt qu'une simple conception philosophique. Cela rend extrêmement difficile de formuler un avis consensuel sur le progrès, car le respect du fait est devenu le critère de vérité, empêchant ainsi de remettre en question les structures de la civilisation et de se lancer dans une révolution nécessaire.\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bef9de-7e61-4c9a-9910-16672126b904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
